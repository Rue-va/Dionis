Assuming unrestricted shared filesystem usage.
host: DESKTOP-OB4VAQ4
Building DAG of jobs...
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                count
---------------  -------
all                    1
consume_kafka          1
generate_report        1
process_audio          1
scrape_species         1
total                  5

Select jobs to execute...
Execute 1 jobs...
[Fri Jan 30 21:41:39 2026]
localrule scrape_species:
    output: species.done
    jobid: 4
    reason: Code has changed since last execution
    resources: tmpdir=C:\Users\Korisnik\AppData\Local\Temp
RuleException:
CalledProcessError in file "C:\Users\Korisnik\OneDrive - A1 Telekom Austria AG\Desktop\dionis_pipeline\Snakefile", line 8:
Command 'python species_scrappe.py && echo done > species.done' returned non-zero exit status 9009.
[Fri Jan 30 21:41:39 2026]
Error in rule scrape_species:
    message: None
    jobid: 4
    output: species.done
    shell:
        python species_scrappe.py && echo done > species.done
        (command exited with non-zero exit code)
Shutting down, this might take some time.
Exiting because a job execution failed. Look below for error messages
[Fri Jan 30 21:41:39 2026]
Error in rule scrape_species:
    message: None
    jobid: 4
    output: species.done
    shell:
        python species_scrappe.py && echo done > species.done
        (command exited with non-zero exit code)
Complete log(s): C:\Users\Korisnik\OneDrive - A1 Telekom Austria AG\Desktop\dionis_pipeline\.snakemake\log\2026-01-30T214138.956320.snakemake.log
WorkflowError:
At least one job did not complete successfully.
